{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "abd6e059-ab0e-42cd-ac6a-74081a5c50fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import datetime\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "53d3565d-72e4-4aa3-bc66-e0a8edaed514",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imported functions:\n",
      "- activate_gpu()\n",
      "- generate_directories()\n",
      "- build_binary_classification_transfer_learning_model()\n",
      "- build_train_image_data_generators_flow_from_directory()\n",
      "- build_callbacks()\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Imported functions:\")\n",
    "print(\"- activate_gpu()\")\n",
    "print(\"- generate_directories()\")\n",
    "print(\"- build_binary_classification_transfer_learning_model()\")\n",
    "print(\"- build_train_image_data_generators_flow_from_directory()\")\n",
    "print(\"- build_callbacks()\")\n",
    "print(\"- optimize_model()\")\n",
    "print(\"- visualize_loss_accuracy_binary_classification()\")\n",
    "print(\"- build_evaluate_image_data_generators_flow_from_directory()\")\n",
    "print(\"- evaluate_binary_classification_model()\")\n",
    "\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e538065-d46c-4b1e-9cb3-b26f5d6c8023",
   "metadata": {},
   "outputs": [],
   "source": [
    "def activate_gpu():\n",
    "    \n",
    "    tf.keras.backend.clear_session()\n",
    "    gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "    if gpus:\n",
    "        try:\n",
    "            tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
    "            for gpu in gpus:\n",
    "                tf.config.experimental.set_memory_growth(gpu, True)\n",
    "            logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "            print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "        except RuntimeError as e:\n",
    "            print(e)\n",
    "            \n",
    "    return gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d74598e-8b4f-4c26-9b55-fed6d355d61c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_directories(model_name,\n",
    "                         model_type,\n",
    "                         working_directory,\n",
    "                         dataset_directory,\n",
    "                         print_directories = False):\n",
    "    \n",
    "    \"\"\"\n",
    "    model_name (str): ResNet50\n",
    "    model_type (str): Binary\n",
    "    working_directory (str): path\n",
    "    dataset_directory (str): path\n",
    "    print_directories (bool): False\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Create model folder:\n",
    "    model_directory = os.path.join(working_directory, model_name)\n",
    "    model_type_directory = os.path.join(model_directory, model_type)\n",
    "    if os.path.exists(model_directory) == False:\n",
    "        os.mkdir(model_directory)\n",
    "    else:\n",
    "        print(\"Folder\", model_directory, \"already exist\")\n",
    "        \n",
    "    # 2. Create model type folder:\n",
    "    if os.path.exists(model_type_directory) == False:\n",
    "        os.mkdir(model_type_directory)\n",
    "    else:\n",
    "        print(\"Folder\", model_type_directory, \"already exist\")\n",
    "        \n",
    "    # 3. Dataset directories:\n",
    "    train_directory = os.path.join(dataset_directory, \"train\")\n",
    "    validation_directory = os.path.join(dataset_directory, \"validation\")\n",
    "    test_directory = os.path.join(dataset_directory, \"test\")\n",
    "    \n",
    "    # 4. Model store and checkpoint directories:\n",
    "    callback_model_checkpoint_directory = os.path.join(model_type_directory, \"keras_model.weights.{epoch:02d}-{val_binary_accuracy:.4f}-{val_loss:.4f}.hdf5\")\n",
    "    callback_tensorboard_directory = os.path.join(model_type_directory, \"logs\")\n",
    "    callback_csv_logger_directory = os.path.join(model_type_directory, \"_\".join([datetime.datetime.now().strftime(\"%d-%m-%Y %H-%M-%S\"), model_name, \"model_optimization_logger.csv\"]))\n",
    "    \n",
    "    # 5. Print directories:\n",
    "    if print_directories:\n",
    "        print(\"Working directory:\", working_directory)\n",
    "        print(\"Model directory:\", model_directory)\n",
    "        print(\"Model type directory:\", model_type_directory)\n",
    "        print(\"Dataset directory:\", dataset_directory)\n",
    "        print(\"Train dataset directory:\", train_directory)\n",
    "        print(\"Validation dataset directory:\", validation_directory)\n",
    "        print(\"Test dataset directory:\", test_directory)\n",
    "        print(\"Model callbacks checkpoint directory:\", callback_model_checkpoint_directory)\n",
    "        print(\"Model callbacks tensorboard directory:\", callback_tensorboard_directory)\n",
    "        print(\"Model callbacks csv logger directory:\", callback_csv_logger_directory)\n",
    "    \n",
    "    directories = dict(working_directory = working_directory,\n",
    "                       model_directory = model_directory,\n",
    "                       model_type_directory = model_type_directory,\n",
    "                       dataset_directory = dataset_directory,\n",
    "                       train_directory = train_directory,\n",
    "                       validation_directory = validation_directory,\n",
    "                       test_directory = test_directory,\n",
    "                       callback_model_checkpoint_directory = callback_model_checkpoint_directory,\n",
    "                       callback_tensorboard_directory = callback_tensorboard_directory,\n",
    "                       callback_csv_logger_directory = callback_csv_logger_directory)\n",
    "    \n",
    "    return directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf1db944-8d40-416b-850a-0b5a1262b7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_binary_classification_transfer_learning_model(model_name,\n",
    "                                                        weights = \"imagenet\",\n",
    "                                                        include_top = False,\n",
    "                                                        print_model_summary = False):\n",
    "    \n",
    "    \"\"\"\n",
    "    model_name (str): \"EfficientNetB0\",\n",
    "                      \"EfficientNetB1\",\n",
    "                      \"EfficientNetB2\",\n",
    "                      \"EfficientNetB3\", \n",
    "                      \"EfficientNetB4\",\n",
    "                      \"EfficientNetB5\", \n",
    "                      \"EfficientNetB6\",\n",
    "                      \"EfficientNetB7\",\n",
    "                      \"Xception\",\n",
    "                      \"VGG16\",\n",
    "                      \"VGG19\",\n",
    "                      \"ResNet50\",\n",
    "                      \"ResNet50V2\",\n",
    "                      \"ResNet101\",\n",
    "                      \"ResNet101V2\", \n",
    "                      \"ResNet152\", \n",
    "                      \"ResNet152V2\", \n",
    "                      \"MobileNet\", \n",
    "                      \"MobileNetV2\":\n",
    "                      \"InceptionV3\", \n",
    "                      \"InceptionResNetV2\",\n",
    "                      \"DenseNet121\", \n",
    "                      \"DenseNet169\", \n",
    "                      \"DenseNet201\", \n",
    "                      \"NASNetLarge\",\n",
    "                      \"NASNetMobile\"\n",
    "    weights (str): \"imagenet\"\n",
    "    include_top (bool): False\n",
    "    print_model_summary (bool): False\n",
    "    \"\"\"\n",
    "    \n",
    "    if model_name == \"EfficientNetB0\":\n",
    "        input_shape = (224, 224, 3)\n",
    "        model_base = tf.keras.applications.EfficientNetB0(include_top = include_top, weights = \"imagenet\", input_shape = input_shape)\n",
    "\n",
    "    if model_name == \"EfficientNetB1\":\n",
    "        input_shape = (240, 240, 3)\n",
    "        model_base = tf.keras.applications.EfficientNetB1(include_top = include_top, weights = \"imagenet\", input_shape = input_shape)\n",
    "\n",
    "    if model_name == \"EfficientNetB2\":\n",
    "        input_shape = (260, 260, 3)\n",
    "        model_base = tf.keras.applications.EfficientNetB2(include_top = include_top, weights = \"imagenet\", input_shape = input_shape)\n",
    "\n",
    "    if model_name == \"EfficientNetB3\":\n",
    "        input_shape = (300, 300, 3)\n",
    "        model_base = tf.keras.applications.EfficientNetB3(include_top = include_top, weights = \"imagenet\", input_shape = input_shape)\n",
    "\n",
    "    if model_name == \"EfficientNetB4\":\n",
    "        input_shape = (380, 380, 3)\n",
    "        model_base = tf.keras.applications.EfficientNetB4(include_top = include_top, weights = \"imagenet\", input_shape = input_shape)\n",
    "\n",
    "    if model_name == \"EfficientNetB5\":\n",
    "        input_shape = (456, 456, 3)\n",
    "        model_base = tf.keras.applications.EfficientNetB5(include_top = include_top, weights = \"imagenet\", input_shape = input_shape) \n",
    "\n",
    "    if model_name == \"EfficientNetB6\":\n",
    "        input_shape = (528, 528, 3)\n",
    "        model_base = tf.keras.applications.EfficientNetB6(include_top = include_top, weights = \"imagenet\", input_shape = input_shape)\n",
    "\n",
    "    if model_name == \"EfficientNetB7\":\n",
    "        input_shape = (600, 600, 3)\n",
    "        model_base = tf.keras.applications.EfficientNetB7(include_top = include_top, weights = \"imagenet\", input_shape = input_shape)\n",
    "\n",
    "    if model_name == \"Xception\":\n",
    "        input_shape = (299, 299, 3)\n",
    "        model_base = tf.keras.applications.Xception(include_top = include_top, weights = \"imagenet\", input_shape = input_shape)\n",
    "\n",
    "    if model_name == \"VGG16\":\n",
    "        input_shape = (224, 224, 3)\n",
    "        model_base = tf.keras.applications.VGG16(include_top = include_top, weights = \"imagenet\", input_shape = input_shape)\n",
    "\n",
    "    if model_name == \"VGG19\":\n",
    "        input_shape = (224, 224, 3)\n",
    "        model_base = tf.keras.applications.VGG19(include_top = include_top, weights = \"imagenet\", input_shape = input_shape)\n",
    "\n",
    "    if model_name == \"ResNet50\":\n",
    "        input_shape = (224, 224, 3)\n",
    "        model_base = tf.keras.applications.ResNet50(include_top = include_top, weights = \"imagenet\", input_shape = input_shape)  \n",
    "\n",
    "    if model_name == \"ResNet50V2\":\n",
    "        input_shape = (224, 224, 3)\n",
    "        model_base = tf.keras.applications.ResNet50V2(include_top = include_top, weights = \"imagenet\", input_shape = input_shape)  \n",
    "\n",
    "    if model_name == \"ResNet101\":\n",
    "        input_shape = (224, 224, 3)\n",
    "        model_base = tf.keras.applications.ResNet101(include_top = include_top, weights = \"imagenet\", input_shape = input_shape)  \n",
    "\n",
    "    if model_name == \"ResNet101V2\":\n",
    "        input_shape = (224, 224, 3)\n",
    "        model_base = tf.keras.applications.ResNet101V2(include_top = include_top, weights = \"imagenet\", input_shape = input_shape) \n",
    "\n",
    "    if model_name == \"ResNet152\":\n",
    "        input_shape = (224, 224, 3)\n",
    "        model_base = tf.keras.applications.ResNet152(include_top = include_top, weights = \"imagenet\", input_shape = input_shape)  \n",
    "\n",
    "    if model_name == \"ResNet152V2\":\n",
    "        input_shape = (224, 224, 3)\n",
    "        model_base = tf.keras.applications.ResNet152V2(include_top = include_top, weights = \"imagenet\", input_shape = input_shape)  \n",
    "\n",
    "    if model_name == \"MobileNet\":\n",
    "        input_shape = (224, 224, 3)\n",
    "        model_base = tf.keras.applications.MobileNet(include_top = include_top, weights = \"imagenet\", input_shape = input_shape)      \n",
    "\n",
    "    if model_name == \"MobileNetV2\":\n",
    "        input_shape = (224, 224, 3)\n",
    "        model_base = tf.keras.applications.MobileNetV2(include_top = include_top, weights = \"imagenet\", input_shape = input_shape)    \n",
    "\n",
    "    if model_name == \"InceptionV3\":\n",
    "        input_shape = (299, 299, 3)\n",
    "        model_base = tf.keras.applications.InceptionV3(include_top = include_top, weights = \"imagenet\", input_shape = input_shape)      \n",
    "\n",
    "    if model_name == \"InceptionResNetV2\":\n",
    "        input_shape = (299, 299, 3)\n",
    "        model_base = tf.keras.applications.InceptionResNetV2(include_top = include_top, weights = \"imagenet\", input_shape = input_shape)     \n",
    "\n",
    "    if model_name == \"DenseNet121\":\n",
    "        input_shape = (224, 224, 3)\n",
    "        model_base = tf.keras.applications.DenseNet121(include_top = include_top, weights = \"imagenet\", input_shape = input_shape)\n",
    "\n",
    "    if model_name == \"DenseNet169\":\n",
    "        input_shape = (224, 224, 3)\n",
    "        model_base = tf.keras.applications.DenseNet169(include_top = include_top, weights = \"imagenet\", input_shape = input_shape)\n",
    "\n",
    "    if model_name == \"DenseNet201\":\n",
    "        input_shape = (224, 224, 3)\n",
    "        model_base = tf.keras.applications.DenseNet201(include_top = include_top, weights = \"imagenet\", input_shape = input_shape)\n",
    "\n",
    "    if model_name == \"NASNetLarge\":\n",
    "        input_shape = (331, 331, 3)\n",
    "        model_base = tf.keras.applications.NASNetLarge(include_top = include_top, weights = \"imagenet\", input_shape = input_shape) \n",
    "\n",
    "    if model_name == \"NASNetMobile\":\n",
    "        input_shape = (224, 224, 3)\n",
    "        model_base = tf.keras.applications.NASNetMobile(include_top = include_top, weights = \"imagenet\", input_shape = input_shape)  \n",
    "\n",
    "    output_tensor = tf.keras.layers.GlobalAveragePooling2D()(model_base.layers[-1].output)\n",
    "    output_tensor = tf.keras.layers.Dense(units = 1, activation = tf.keras.activations.sigmoid)(output_tensor)\n",
    "    model = tf.keras.models.Model(inputs = model_base.inputs, outputs = output_tensor)\n",
    "    \n",
    "    if print_model_summary:\n",
    "        print(\"Model name:\", model_name)\n",
    "        print(model.summary())\n",
    "    \n",
    "    return(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8b2e1da9-fd3d-4d12-96d9-8b46a496b054",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_train_image_data_generators_flow_from_directory(model,\n",
    "                                                          directories,\n",
    "                                                          parameters,\n",
    "                                                          augmentation_parameters = None):\n",
    "    \n",
    "    \"\"\"\n",
    "    model (obj): model\n",
    "    directories (dict) : directories\n",
    "    parameters (dict) : parameters\n",
    "    augmentation_parameters (dict) : augmentation_parameters\n",
    "    \"\"\"\n",
    "    \n",
    "    if augmentation_parameters:\n",
    "        rescale = dict(rescale = 1/255)\n",
    "        augmentation_parameters = {**augmentation_parameters, **rescale}\n",
    "    else:\n",
    "        augmentation_parameters = dict(rescale = 1/255)\n",
    "        \n",
    "    print(\"Augmentation parameters:\")\n",
    "    for k, v in augmentation_parameters.items():\n",
    "        print(\"->\", k, \":\", v)\n",
    "    print(\"\")\n",
    "        \n",
    "    train_data_generator = tf.keras.preprocessing.image.ImageDataGenerator(**augmentation_parameters)\n",
    "    validation_data_generator = tf.keras.preprocessing.image.ImageDataGenerator(rescale = 1/255)\n",
    "    \n",
    "    train_data_generator = train_data_generator.flow_from_directory(directory = directories[\"train_directory\"],\n",
    "                                                                    target_size = (model.input_shape[1], model.input_shape[1]),\n",
    "                                                                    batch_size = parameters[\"batch_size\"],\n",
    "                                                                    class_mode = parameters[\"class_mode\"],\n",
    "                                                                    shuffle = parameters[\"shuffle\"])\n",
    "    \n",
    "    validation_data_generator = validation_data_generator.flow_from_directory(directory = directories[\"validation_directory\"],\n",
    "                                                                              target_size = (model.input_shape[1], model.input_shape[1]),\n",
    "                                                                              batch_size = parameters[\"batch_size\"],\n",
    "                                                                              class_mode = parameters[\"class_mode\"],\n",
    "                                                                              shuffle = parameters[\"shuffle\"])\n",
    "    \n",
    "    train_data_generators = dict(train_data_generator = train_data_generator,\n",
    "                                 validation_data_generator = validation_data_generator)\n",
    "    \n",
    "    return train_data_generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "13ff04af-5057-4c70-bd32-77fb050facd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.engine.functional.Functional at 0x1cd87feb048>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def build_callbacks(directories,\n",
    "                    parameters):\n",
    "    \n",
    "    \"\"\"\n",
    "    directories (dict) : directories\n",
    "    parameters (dict) : parameters\n",
    "    \"\"\"\n",
    "\n",
    "    callbacks = [\n",
    "        tf.keras.callbacks.ModelCheckpoint(\n",
    "            filepath = directories[\"callback_model_checkpoint_directory\"],\n",
    "            monitor = parameters[\"monitor\"],\n",
    "            verbose = parameters[\"verbose\"],\n",
    "            save_best_only = parameters[\"save_best_only\"]),\n",
    "        tf.keras.callbacks.EarlyStopping(\n",
    "            monitor = parameters[\"monitor\"],\n",
    "            verbose = parameters[\"verbose\"],\n",
    "            patience = parameters[\"early_stopping_patience\"],\n",
    "            restore_best_weights = parameters[\"restore_best_weights\"]),\n",
    "        tf.keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor = parameters[\"monitor\"],\n",
    "            verbose = parameters[\"verbose\"]),\n",
    "        tf.keras.callbacks.TensorBoard(\n",
    "            log_dir = directories[\"callback_tensorboard_directory\"]),\n",
    "        tf.keras.callbacks.CSVLogger(\n",
    "            filename = directories[\"callback_csv_logger_directory\"],\n",
    "            separator = \";\",\n",
    "            append = True)\n",
    "    ]\n",
    "    \n",
    "    return callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114105cf-9c20-4c4f-82bc-f8b8deb9ffa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model(model,\n",
    "                   train_data_generators,\n",
    "                   parameters,\n",
    "                   callbacks):\n",
    "    \n",
    "    \"\"\"\n",
    "    model (obj) : model\n",
    "    train_data_generators (dict) : train_data_generators\n",
    "    parameters (dict) : parameters\n",
    "    callbacks (list) : callbacks\n",
    "    \"\"\"\n",
    "    \n",
    "    history = model.fit_generator(\n",
    "        generator = train_data_generators[\"train_data_generator\"],\n",
    "        steps_per_epoch = np.ceil(train_data_generators[\"train_data_generator\"].n/train_data_generators[\"train_data_generator\"].batch_size),\n",
    "        epochs = parameters[\"epochs\"],\n",
    "        validation_data = train_data_generators[\"validation_data_generator\"],\n",
    "        validation_steps = np.ceil(train_data_generators[\"validation_data_generator\"].n/train_data_generators[\"validation_data_generator\"].batch_size),\n",
    "        callbacks = callbacks       \n",
    "    )\n",
    "    \n",
    "    results = pd.DataFrame(history.history)\n",
    "    results[\"epoch\"] = results.index + 1\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a5e233f-001e-45a2-ab11-fbd8d66a5424",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_loss_accuracy_binary_classification(history):\n",
    "    \n",
    "    \"\"\"\n",
    "    history (dataframe) : history\n",
    "    \"\"\"\n",
    "    \n",
    "    min_loss = history[history[\"val_loss\"] == min(history[\"val_loss\"])]\n",
    "    max_loss = history[history[\"val_loss\"] == max(history[\"val_loss\"])]\n",
    "    min_accuracy = history[history[\"val_binary_accuracy\"] == min(history[\"val_binary_accuracy\"])]\n",
    "    max_accuracy = history[history[\"val_binary_accuracy\"] == max(history[\"val_binary_accuracy\"])]\n",
    "    epoch = history[history[\"epoch\"] == max(history[\"epoch\"])][\"epoch\"].iloc[0]\n",
    "\n",
    "    plt.rcParams['figure.figsize'] = [25, 5]\n",
    "\n",
    "    fig, ax = plt.subplots(1, 2)\n",
    "    ax[0].plot(history[\"epoch\"], \n",
    "               history[\"loss\"], \n",
    "               color = \"blue\", \n",
    "               marker = \"o\", \n",
    "               linestyle = \"--\",\n",
    "               label = 'Loss')\n",
    "    ax[0].plot(history[\"epoch\"],\n",
    "               history[\"val_loss\"], \n",
    "               color = \"red\", \n",
    "               marker = \"o\",\n",
    "               linestyle = \"--\", \n",
    "               label = 'Val Loss')\n",
    "    ax[0].legend(loc = \"upper right\")\n",
    "    ax[0].set_xlabel(\"Epoch\")\n",
    "    ax[0].set_ylabel(\"Loss\")\n",
    "    ax[0].set_title(\"Loss plot\")\n",
    "\n",
    "    ax[0].annotate(text = \"Val Loss = \" + str(round(min_loss[\"val_loss\"].iloc[0], 2)), \n",
    "                   xy = (min_loss[\"epoch\"], min_loss[\"val_loss\"]),\n",
    "                   xytext = (epoch/2, 0.5 * (min_loss[\"val_loss\"].iloc[0] + max_loss[\"val_loss\"].iloc[0])),\n",
    "                   arrowprops = {\"arrowstyle\": \"->\", \"color\": \"black\"})\n",
    "    ax[0].grid()\n",
    "\n",
    "    ax[1].plot(history[\"epoch\"], \n",
    "               history[\"binary_accuracy\"], \n",
    "               color = \"blue\",\n",
    "               marker = \"o\", \n",
    "               linestyle = \"--\", \n",
    "               label = 'Accuracy')\n",
    "    ax[1].plot(history[\"epoch\"],\n",
    "               history[\"val_binary_accuracy\"],\n",
    "               color = \"red\",\n",
    "               marker = \"o\",\n",
    "               linestyle = \"--\", \n",
    "               label = 'Val Accuracy')\n",
    "    ax[1].legend(loc = \"upper left\")\n",
    "    ax[1].set_xlabel(\"Epoch\")\n",
    "    ax[1].set_ylabel(\"Accuracy\")\n",
    "    ax[1].set_title(\"Accuracy plot\")\n",
    "    ax[1].annotate(text = \"Val Accuracy = \" + str(round(max_accuracy[\"val_binary_accuracy\"].iloc[0], 2)), \n",
    "                   xy = (max_accuracy[\"epoch\"], max_accuracy[\"val_binary_accuracy\"]),\n",
    "                   xytext = (epoch/2, 0.5 * (min_accuracy[\"val_binary_accuracy\"].iloc[0] + max_accuracy[\"val_binary_accuracy\"].iloc[0])),\n",
    "                   arrowprops = {\"arrowstyle\": \"->\", \"color\": \"black\"})\n",
    "    ax[1].grid()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "433560c7-a535-4af9-a9f2-96ff85761193",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_evaluate_image_data_generators_flow_from_directory(model,\n",
    "                                                             directories,\n",
    "                                                             parameters):\n",
    "    \n",
    "    \"\"\"\n",
    "    model (obj): model\n",
    "    directories (dict) : directories\n",
    "    parameters (dict) : parameters\n",
    "    \"\"\"\n",
    "\n",
    "    train_data_generator = tf.keras.preprocessing.image.ImageDataGenerator(rescale = 1/255)\n",
    "    validation_data_generator = tf.keras.preprocessing.image.ImageDataGenerator(rescale = 1/255)\n",
    "    test_data_generator = tf.keras.preprocessing.image.ImageDataGenerator(rescale = 1/255)\n",
    "    \n",
    "    train_data_generator = train_data_generator.flow_from_directory(directory = directories[\"train_directory\"],\n",
    "                                                                    target_size = (model.input_shape[1], model.input_shape[1]),\n",
    "                                                                    batch_size = parameters[\"batch_size\"],\n",
    "                                                                    class_mode = parameters[\"class_mode\"],\n",
    "                                                                    shuffle = False)\n",
    "    \n",
    "    validation_data_generator = validation_data_generator.flow_from_directory(directory = directories[\"validation_directory\"],\n",
    "                                                                              target_size = (model.input_shape[1], model.input_shape[1]),\n",
    "                                                                              batch_size = parameters[\"batch_size\"],\n",
    "                                                                              class_mode = parameters[\"class_mode\"],\n",
    "                                                                              shuffle = False)\n",
    "    \n",
    "    test_data_generator = test_data_generator.flow_from_directory(directory = directories[\"test_directory\"],\n",
    "                                                                  target_size = (model.input_shape[1], model.input_shape[1]),\n",
    "                                                                  batch_size = parameters[\"batch_size\"],\n",
    "                                                                  class_mode = parameters[\"class_mode\"],\n",
    "                                                                  shuffle = False)\n",
    "    \n",
    "    evaluate_data_generators = dict(train_data_generator = train_data_generator,\n",
    "                                    validation_data_generator = validation_data_generator,\n",
    "                                    test_data_generator = test_data_generator)\n",
    "    \n",
    "    return evaluate_data_generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ed4104-d9ae-4e96-a809-3534d2a2de08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_binary_classification_model(model,\n",
    "                                         evaluate_data_generators):\n",
    "    \n",
    "    \"\"\"\n",
    "    model (obj) : model\n",
    "    evaluate_data_generators (dict) : evaluate_data_generators\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Train generator evaluation\")\n",
    "    train_evaluation = model.evaluate_generator(\n",
    "        generator = evaluate_data_generators[\"train_data_generator\"], \n",
    "        steps = np.ceil(evaluate_data_generators[\"train_data_generator\"].n/evaluate_data_generators[\"train_data_generator\"].batch_size)\n",
    "    )\n",
    "    \n",
    "    print(\"Validation generator evaluation\")\n",
    "    validation_evaluation = model.evaluate_generator(\n",
    "        generator = evaluate_data_generators[\"validation_data_generator\"], \n",
    "        steps = np.ceil(evaluate_data_generators[\"validation_data_generator\"].n/evaluate_data_generators[\"validation_data_generator\"].batch_size)\n",
    "    )\n",
    "    \n",
    "    print(\"Test generator evaluation\")\n",
    "    test_evaluation = model.evaluate_generator(\n",
    "        generator = evaluate_data_generators[\"test_data_generator\"], \n",
    "        steps = np.ceil(evaluate_data_generators[\"test_data_generator\"].n/evaluate_data_generators[\"test_data_generator\"].batch_size)\n",
    "    )\n",
    "\n",
    "    evaluation = pd.DataFrame({\"Train\" : train_evaluation,\n",
    "                               \"Validation\" : validation_evaluation,\n",
    "                               \"Test\" : test_evaluation},\n",
    "                               index = [\"Loss\", \"Accuracy\"])\n",
    "    \n",
    "    return evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62bf5a1f-51c3-40e6-a5ae-85d32ae347ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90541354-07c4-4605-aaff-abc6244c3cc1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GPU_ML_TF",
   "language": "python",
   "name": "gpu_ml_tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
